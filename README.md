
# Meda

## Description
This is an experimental program I made that uses LLM-fueled (BERT) semantic search and topic modeling in order to maintain, organize, and search a user-created, flat collection of directories with heterogeneous contents. It's deliberately _not_ production-ready; for example, every time you run this program it has to load the `sentence-BERT` model in Python, compile functions, etc, which is very inefficient for a small program like this that's designed to be run frequently from the command line. It's just intended as a proof of concept and as a working solution until something better comes along.

The semantic search aspect of this works very well, in my experience. The topic modeling part (based on a hierarchical clustering of PCA-reduced embeddings) is less successful, but good enough; in the test data I show that it fairly well recovers the original 7 topics that they were generated from (more on that below).

## Motivation
I deal with a lot of projects of an exploratory or unpredictable nature. Some of them mature into bigger or more stable projects. Those that don't, there may still be value, but it may be unclear how to organize them, what are the common threads, etc. On top of that, there are innumerable things like datasets, models, course materials, notes, etc, that I'd like to keep track of. I've increasingly found that a traditional hierarchical organization for this kind of thing is not very satisfactory, for several reasons such as:
- inflexible: while you may have chosen a good hierarchy for your content in the beginning, those choices will (in my experience) always come back to haunt you, leaving with you the burden of rearranging things or else living with that suboptimal arrangement. The former approach, rearranging, does little more than guarantee you'll continually revisit this same problem.
- complex: if you have a fairly complex set of contents to manage, a hierarchical approach can create deep mazes of directories. This harms education (familiarizing new users with the file structure), retrieval (file paths become long and unwieldy), etc.

My solution is to store practically everything that may be of a transient or unpredictable nature in a single directory called `contents`. By "almost everything", I mean datasets, media, documents, code, etc. I make no distinction among those things, but rather leave it to metadata to capture the nature of groupings among things, in such a way that a semantic search can group the content for me, dynamically according to my needs in any given use case. Also there is almost no nesting of directories within `contents`; the only exception is where there's a very strong organizational convention, such as grouping some code's contents into subfolders like `assets`, `src`, and `docs`, etc.


## Installation
This package works best by building it into an relocatable binary.

First, clone the repo or install it as a package in Julia:
```
using Pkg
Pkg.install(url = "https://github.com/myersm0/Meda.jl")
```

Then at the root directory of the project, start Julia with `julia --project` and then:
```
using PackageCompiler
create_app(".", "Meda")
```

## Usage
In this repo under `test/data/content/`, I have a collection of 70 example directories, each with a `meta.json` file having a statement of purpose (generated by GPT-4) on one of 7 distinct topics.

If I type:
```bash
Meda find --query="great developments in technology" --base_dir="./test/data/content/"
```

I get the following (abbreviated) result:
> ▪ 452cea: JavaScript snippets and libraries for interactive web applications
> ▪ 0705d1: Responsive web design tutorials and best practices
> ▪ fe7a1d: React components and hooks for front-end development
>   … (omitting 7 results)
>
> ▪ **d58ad0**: Gallery of restored photographs from the Industrial Revolution
> ▪ 69b7db: Collection of advanced post-processing techniques in Adobe Photoshop
> ▪ 4cfa56: Guide to vintage camera collection and maintenance
>   … (omitting 3 results)
>
> ▪ c28593: Compilation of quick and easy 30-minute meals
> ▪ 192ae6: Dietary advice and recipes for weight loss and muscle gain
> ▪ 4d30e0: Nutritional guides and meal plans for athletes
>   … (omitting 5 results)
>
> ▪ **b2054c**: Interactive timeline of significant scientific discoveries
> ▪ **22610e**: Space exploration timelines and historical documents
> ▪ **69575d**: Audiovisual materials on the space race of the 20th century
>   … (omitting 12 results)

In each line in the result, we see a hexadecimal key (a random name for the folder containing some content) and a description of the contents of that folder. What's happening here is that the 70 `meta.json` files from the testing set are parsed and clustered into `k` topics. Each topic is sorted by relevance to my query, "great developments in technology." Those results that are among the `top_n` most relevant to my query are given in bold. For each topic, up to `n_per_group` results are shown, and then the rest are elided. (For brevity, only four of the seven topics are shown above.)

## TODO
There are several obvious next steps but which unfortunately I probably won't have the bandwidth to implement myself:
- create a dependency graph of relationships among items
- add comments from code into the body of metadata that gets searched
